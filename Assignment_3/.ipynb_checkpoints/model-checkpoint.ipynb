{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import functools\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tfe.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(tfe.Network):\n",
    "    \n",
    "    def __init__(self,cub_siz,pub_siz,hist_len,logits_units,num_filt1=32,kernel_size1=5,num_filt2=64,kernel_size2=5,\n",
    "                 pool_size=2,dense_units=1024,dropout_rate=0.4):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self._input_shape = [-1, cub_siz*pub_siz, cub_siz*pub_siz, hist_len]\n",
    "        self.conv1 = self.track_layer(tf.layers.Conv2D(filters=num_filt1,kernel_size=kernel_size1,padding=\"same\",activation=tf.nn.relu))\n",
    "        self.maxpool1 = self.track_layer(tf.layers.MaxPooling2D(pool_size=pool_size, strides=2))\n",
    "        self.conv2 = self.track_layer(tf.layers.Conv2D(filters=num_filt2,kernel_size=kernel_size2,padding=\"same\",activation=tf.nn.relu))\n",
    "        self.maxpool2 = self.track_layer(tf.layers.MaxPooling2D(pool_size=pool_size, strides=2))\n",
    "        self.dense1 = self.track_layer(tf.layers.Dense(units=dense_units, activation=tf.nn.relu))\n",
    "        self.dropoutlayer = self.track_layer(tf.layers.Dropout(rate=dropout_rate))\n",
    "        self.logits = self.track_layer(tf.layers.Dense(units=logits_units))\n",
    "        \n",
    "        ds = tf.data.Dataset.from_tensor_slices((np.zeros((1,2500),dtype=np.float32),np.zeros((1,5),np.int32)))\n",
    "        for (image,label) in tfe.Iterator(ds):\n",
    "            self.predict(image,training=False)\n",
    "    \n",
    "    def predict(self, inputs, training):\n",
    "        \"\"\"Actually runs the model.\"\"\"\n",
    "        input_layer = tf.reshape(inputs, self._input_shape)\n",
    "        result = self.conv1(input_layer)\n",
    "        result = self.maxpool1(result)\n",
    "        result = self.conv2(result)\n",
    "        result = self.maxpool2(result)\n",
    "        result = tf.layers.flatten(result)\n",
    "        if training:\n",
    "            result = self.dropoutlayer(result)\n",
    "        result = self.logits(result)\n",
    "        return result\n",
    "    \n",
    "    def loss(self, predictions, labels):\n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=labels))\n",
    "    \n",
    "    def compute_accuracy(self, predictions, labels):\n",
    "        return tf.reduce_sum(tf.cast(tf.equal(\n",
    "              tf.argmax(predictions, axis=1,\n",
    "                        output_type=tf.int64),\n",
    "              tf.argmax(labels, axis=1,\n",
    "                        output_type=tf.int64)),\n",
    "                dtype=tf.float32)) / float(predictions.shape[0].value)\n",
    "    \n",
    "    def train_one_epoch(self, optimizer, dataset, n_minibatches, log_interval):\n",
    "        \"\"\"Trains model on `dataset` using `optimizer`.\"\"\"\n",
    "        tf.train.get_or_create_global_step()\n",
    "\n",
    "        def model_loss(images, labels):\n",
    "            prediction = self.predict(images, training=True)\n",
    "            loss_value = self.loss(prediction, labels)\n",
    "            tf.contrib.summary.scalar('loss', loss_value)\n",
    "            tf.contrib.summary.scalar('accuracy',\n",
    "                                      self.compute_accuracy(prediction, labels))\n",
    "            return loss_value\n",
    "\n",
    "        for (batch, (images, labels)) in enumerate(tfe.Iterator(dataset)):\n",
    "            with tf.contrib.summary.record_summaries_every_n_global_steps(10):\n",
    "                batch_model_loss = functools.partial(model_loss, images, labels)\n",
    "                optimizer.minimize(batch_model_loss, global_step=tf.train.get_global_step())\n",
    "            if log_interval and batch % log_interval == 0:\n",
    "                print('Batch #%d\\tLoss: %.6f' % (batch, batch_model_loss()))\n",
    "            if batch == n_minibatches-1:\n",
    "                return\n",
    "    \n",
    "    def test(self, dataset):\n",
    "        \"\"\"Perform an evaluation of `model` on the examples from `dataset`.\"\"\"\n",
    "        avg_loss = tfe.metrics.Mean('loss')\n",
    "        accuracy = tfe.metrics.Accuracy('accuracy')\n",
    "        \n",
    "        for (images, labels) in tfe.Iterator(dataset):\n",
    "            predictions = self.predict(images, training=False)\n",
    "            avg_loss(self.loss(predictions, labels))\n",
    "            accuracy(tf.argmax(labels, axis=1, output_type=tf.int64),tf.argmax(predictions, axis=1, output_type=tf.int64))\n",
    "            break\n",
    "        print('Validation set: Average loss: %.4f, Accuracy: %.2f%%\\n' % (avg_loss.result(), 100 * accuracy.result()))\n",
    "        with tf.contrib.summary.always_record_summaries():\n",
    "            tf.contrib.summary.scalar('loss', avg_loss.result())\n",
    "            tf.contrib.summary.scalar('accuracy', accuracy.result())\n",
    "    \n",
    "    def train(self,X_train,y_train,X_valid,y_valid,batch_size=50,n_minibatches=500,num_epochs=10,learning_rate=1e-3,\n",
    "              dataset_size=None,log_interval=None,no_gpu=True,checkpoint_dir=None):\n",
    "        \n",
    "        if no_gpu or tfe.num_gpus() <= 0:\n",
    "            device = '/cpu:0'\n",
    "        else:\n",
    "            device = '/gpu:0'\n",
    "        print('Using device %s.' % (device))\n",
    "        \n",
    "        # create appropriate datasets\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train,y_train))\n",
    "        valid_ds = tf.data.Dataset.from_tensor_slices((X_valid,y_valid))\n",
    "        if dataset_size == None:\n",
    "            dataset_size = X_train.shape[0]\n",
    "        train_ds = train_ds.shuffle(dataset_size).batch(batch_size)\n",
    "        valid_ds = valid_ds.batch(X_valid.shape[0])\n",
    "        \n",
    "        # create optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "        \n",
    "        # store output during training\n",
    "        out_dir = '\\\\tmp\\\\tensorflow\\\\NeuralPlanner\\\\output'\n",
    "        \n",
    "        train_dir = os.path.join(out_dir,'train')\n",
    "        test_dir = os.path.join(out_dir,'eval')\n",
    "        tf.gfile.MakeDirs(out_dir)\n",
    "        summary_writer = tf.contrib.summary.create_file_writer(train_dir,flush_millis=10000)\n",
    "        test_summary_writer = tf.contrib.summary.create_file_writer(test_dir,flush_millis=10000,name='test')\n",
    "        dir_copy = checkpoint_dir\n",
    "        checkpoint_dir = os.path.join(dir_copy,'train')\n",
    "        checkpoint_dir_save = os.path.join(dir_copy,'save')\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "        checkpoint_prefix_save = os.path.join(checkpoint_dir_save, 'ckpt')\n",
    "        \n",
    "        # store data attributed to learning curve \n",
    "        training_accuracy = np.zeros(num_epochs)\n",
    "        training_loss = np.zeros(num_epochs)\n",
    "        valid_accuracy = np.zeros(num_epochs)\n",
    "        valid_loss = np.zeros(num_epochs)\n",
    "        \n",
    "        with tf.device(device):\n",
    "            for epoch in range(1,num_epochs+1):\n",
    "                # return should be learning curve data\n",
    "                curr_pred_train = self.predict(X_train,training=False)\n",
    "                curr_pred_valid = self.predict(X_valid,training=False)\n",
    "                training_accuracy[epoch-1] = self.compute_accuracy(curr_pred_train,y_train)\n",
    "                training_loss[epoch-1] = self.loss(curr_pred_train,y_train)\n",
    "                valid_accuracy[epoch-1] = self.compute_accuracy(curr_pred_valid,y_valid)\n",
    "                valid_loss[epoch-1] = self.loss(curr_pred_valid,y_valid)\n",
    "                \n",
    "                with tfe.restore_variables_on_create(tf.train.latest_checkpoint(checkpoint_dir)):\n",
    "                    global_step = tf.train.get_or_create_global_step()\n",
    "                    start = time.time()\n",
    "                    with summary_writer.as_default():\n",
    "                        self.train_one_epoch(optimizer, train_ds, n_minibatches, log_interval)\n",
    "                    end = time.time()\n",
    "                    print('\\nTrain time for epoch #%d (global step %d): %f' % (epoch, global_step.numpy(), end - start))\n",
    "                with test_summary_writer.as_default():\n",
    "                    self.test(valid_ds)\n",
    "                all_variables = (self.variables + optimizer.variables() + [global_step])\n",
    "                tfe.Saver(all_variables).save(checkpoint_prefix, global_step=global_step)\n",
    "                tfe.Saver(self.variables).save(checkpoint_prefix_save, global_step=global_step)\n",
    "        return training_accuracy,training_loss,valid_accuracy,valid_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

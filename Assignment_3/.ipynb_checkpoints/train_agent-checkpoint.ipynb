{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tfe.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom modules\n",
    "from utils     import Options\n",
    "from simulator import Simulator\n",
    "from transitionTable import TransitionTable\n",
    "from model import ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states & labels loaded.\n",
      "states stacked w/ history of 4\n",
      "train & valid data splited.\n"
     ]
    }
   ],
   "source": [
    "# 0. initialization\n",
    "opt = Options()\n",
    "sim = Simulator(opt.map_ind, opt.cub_siz, opt.pob_siz, opt.act_num)\n",
    "trans = TransitionTable(opt.state_siz, opt.act_num, opt.hist_len,\n",
    "                             opt.minibatch_size, opt.valid_size,\n",
    "                             opt.states_fil, opt.labels_fil)\n",
    "NeuralPlanner = ConvNet(cub_siz=opt.cub_siz,pub_siz=opt.pob_siz,hist_len=opt.hist_len,logits_units=opt.act_num,\n",
    "                        num_filt1=opt.num_filt1,kernel_size1=opt.kernel_size1,num_filt2=opt.num_filt2,\n",
    "                        kernel_size2=opt.kernel_size2,pool_size=opt.pool_size,dense_units=opt.dense_units,dropout_rate=opt.dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (16000, 2500)\n",
      "Shape of X_valid: (500, 2500)\n"
     ]
    }
   ],
   "source": [
    "# Get the full data from the transition table like this:\n",
    "X_train, y_train = trans.get_train()\n",
    "X_valid, y_valid = trans.get_valid()\n",
    "X_train = np.asarray(X_train, dtype=np.float32)\n",
    "X_valid = np.asarray(X_valid, dtype=np.float32)\n",
    "y_train = np.asarray(y_train, dtype=np.int32)\n",
    "y_valid = np.asarray(y_valid, dtype=np.int32)\n",
    "\n",
    "print('Shape of X_train:', X_train.shape)\n",
    "print('Shape of X_valid:', X_valid.shape)\n",
    "\n",
    "# Hint: to ease loading your model later create a model.py file\n",
    "# where you define your network configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively you can get one random mini batch line this\n",
    "\n",
    "# for i in range(number_of_batches):\n",
    "#     x, y = trans.sample_minibatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device /cpu:0.\n",
      "WARNING:tensorflow:From C:\\Users\\Hakan\\Documents\\Uni_Freiburg\\Semester_3\\Deep-Learning-Lab\\Assignment_3\\model.py:51: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Batch #0\tLoss: 32.630676\n",
      "Batch #10\tLoss: 1.025966\n",
      "Batch #20\tLoss: 0.955685\n",
      "Batch #30\tLoss: 0.649951\n",
      "Batch #40\tLoss: 0.541890\n",
      "Batch #50\tLoss: 0.688399\n",
      "Batch #60\tLoss: 0.560766\n",
      "Batch #70\tLoss: 0.353948\n",
      "Batch #80\tLoss: 0.296296\n",
      "Batch #90\tLoss: 0.274432\n",
      "Batch #100\tLoss: 0.338042\n",
      "Batch #110\tLoss: 0.504536\n",
      "Batch #120\tLoss: 0.301726\n",
      "Batch #130\tLoss: 0.247383\n",
      "Batch #140\tLoss: 0.302006\n",
      "Batch #150\tLoss: 0.212800\n",
      "Batch #160\tLoss: 0.215031\n",
      "Batch #170\tLoss: 0.164073\n",
      "Batch #180\tLoss: 0.267009\n",
      "Batch #190\tLoss: 0.203336\n",
      "Batch #200\tLoss: 0.338935\n",
      "Batch #210\tLoss: 0.332680\n",
      "Batch #220\tLoss: 0.140118\n",
      "Batch #230\tLoss: 0.125359\n",
      "Batch #240\tLoss: 0.228436\n",
      "Batch #250\tLoss: 0.223067\n",
      "Batch #260\tLoss: 0.113722\n",
      "Batch #270\tLoss: 0.093139\n",
      "Batch #280\tLoss: 0.130479\n",
      "Batch #290\tLoss: 0.196041\n",
      "Batch #300\tLoss: 0.145245\n",
      "Batch #310\tLoss: 0.295522\n",
      "Batch #320\tLoss: 0.151528\n",
      "Batch #330\tLoss: 0.125617\n",
      "Batch #340\tLoss: 0.057610\n",
      "Batch #350\tLoss: 0.153286\n",
      "Batch #360\tLoss: 0.055485\n",
      "Batch #370\tLoss: 0.219843\n",
      "Batch #380\tLoss: 0.055212\n",
      "Batch #390\tLoss: 0.100769\n",
      "Batch #400\tLoss: 0.203636\n",
      "Batch #410\tLoss: 0.336339\n",
      "Batch #420\tLoss: 0.153265\n",
      "Batch #430\tLoss: 0.148780\n",
      "Batch #440\tLoss: 0.529026\n",
      "Batch #450\tLoss: 0.345264\n",
      "Batch #460\tLoss: 0.048330\n",
      "Batch #470\tLoss: 0.053361\n",
      "Batch #480\tLoss: 0.090962\n",
      "Batch #490\tLoss: 0.179936\n",
      "\n",
      "Train time for epoch #1 (global step 2000): 85.936494\n",
      "Validation set: Average loss: 0.1579, Accuracy: 95.400000%\n",
      "\n",
      "Batch #0\tLoss: 0.081714\n",
      "Batch #10\tLoss: 0.241449\n",
      "Batch #20\tLoss: 0.059285\n",
      "Batch #30\tLoss: 0.064070\n",
      "Batch #40\tLoss: 0.077774\n",
      "Batch #50\tLoss: 0.087108\n",
      "Batch #60\tLoss: 0.063445\n",
      "Batch #70\tLoss: 0.102440\n",
      "Batch #80\tLoss: 0.058326\n",
      "Batch #90\tLoss: 0.091654\n",
      "Batch #100\tLoss: 0.052330\n",
      "Batch #110\tLoss: 0.047158\n",
      "Batch #120\tLoss: 0.081146\n",
      "Batch #130\tLoss: 0.051559\n",
      "Batch #140\tLoss: 0.059862\n",
      "Batch #150\tLoss: 0.128904\n",
      "Batch #160\tLoss: 0.017897\n",
      "Batch #170\tLoss: 0.084353\n",
      "Batch #180\tLoss: 0.058211\n",
      "Batch #190\tLoss: 0.077446\n",
      "Batch #200\tLoss: 0.090730\n",
      "Batch #210\tLoss: 0.045081\n",
      "Batch #220\tLoss: 0.038909\n",
      "Batch #230\tLoss: 0.074446\n",
      "Batch #240\tLoss: 0.024765\n",
      "Batch #250\tLoss: 0.068594\n",
      "Batch #260\tLoss: 0.069523\n",
      "Batch #270\tLoss: 0.044478\n",
      "Batch #280\tLoss: 0.074357\n",
      "Batch #290\tLoss: 0.034612\n",
      "Batch #300\tLoss: 0.038405\n",
      "Batch #310\tLoss: 0.211796\n",
      "Batch #320\tLoss: 0.037698\n",
      "Batch #330\tLoss: 0.204061\n",
      "Batch #340\tLoss: 0.153381\n",
      "Batch #350\tLoss: 0.075778\n",
      "Batch #360\tLoss: 0.044800\n",
      "Batch #370\tLoss: 0.055929\n",
      "Batch #380\tLoss: 0.184842\n",
      "Batch #390\tLoss: 0.055192\n",
      "Batch #400\tLoss: 0.036157\n",
      "Batch #410\tLoss: 0.101020\n",
      "Batch #420\tLoss: 0.164078\n",
      "Batch #430\tLoss: 0.048390\n",
      "Batch #440\tLoss: 0.132252\n",
      "Batch #450\tLoss: 0.106179\n",
      "Batch #460\tLoss: 0.215554\n",
      "Batch #470\tLoss: 0.080367\n",
      "Batch #480\tLoss: 0.039771\n",
      "Batch #490\tLoss: 0.114764\n",
      "\n",
      "Train time for epoch #2 (global step 2500): 92.031545\n",
      "Validation set: Average loss: 0.1399, Accuracy: 95.800000%\n",
      "\n",
      "Batch #0\tLoss: 0.017316\n",
      "Batch #10\tLoss: 0.140541\n",
      "Batch #20\tLoss: 0.010669\n",
      "Batch #30\tLoss: 0.100405\n",
      "Batch #40\tLoss: 0.106090\n",
      "Batch #50\tLoss: 0.040926\n",
      "Batch #60\tLoss: 0.034494\n",
      "Batch #70\tLoss: 0.172588\n",
      "Batch #80\tLoss: 0.206279\n",
      "Batch #90\tLoss: 0.094888\n",
      "Batch #100\tLoss: 0.121556\n",
      "Batch #110\tLoss: 0.071880\n",
      "Batch #120\tLoss: 0.044684\n",
      "Batch #130\tLoss: 0.101895\n",
      "Batch #140\tLoss: 0.046183\n",
      "Batch #150\tLoss: 0.056632\n",
      "Batch #160\tLoss: 0.168008\n",
      "Batch #170\tLoss: 0.134460\n",
      "Batch #180\tLoss: 0.153257\n",
      "Batch #190\tLoss: 0.086383\n",
      "Batch #200\tLoss: 0.149862\n",
      "Batch #210\tLoss: 0.028898\n",
      "Batch #220\tLoss: 0.090865\n",
      "Batch #230\tLoss: 0.143750\n",
      "Batch #240\tLoss: 0.056853\n",
      "Batch #250\tLoss: 0.068066\n",
      "Batch #260\tLoss: 0.085126\n",
      "Batch #270\tLoss: 0.063340\n",
      "Batch #280\tLoss: 0.055231\n",
      "Batch #290\tLoss: 0.088809\n",
      "Batch #300\tLoss: 0.098294\n",
      "Batch #310\tLoss: 0.058233\n",
      "Batch #320\tLoss: 0.047899\n",
      "Batch #330\tLoss: 0.035643\n",
      "Batch #340\tLoss: 0.142515\n",
      "Batch #350\tLoss: 0.095609\n",
      "Batch #360\tLoss: 0.044607\n",
      "Batch #370\tLoss: 0.075516\n",
      "Batch #380\tLoss: 0.107934\n",
      "Batch #390\tLoss: 0.188821\n",
      "Batch #400\tLoss: 0.105551\n",
      "Batch #410\tLoss: 0.134850\n",
      "Batch #420\tLoss: 0.069440\n",
      "Batch #430\tLoss: 0.011476\n",
      "Batch #440\tLoss: 0.081068\n",
      "Batch #450\tLoss: 0.050866\n",
      "Batch #460\tLoss: 0.158515\n",
      "Batch #470\tLoss: 0.022613\n",
      "Batch #480\tLoss: 0.158386\n",
      "Batch #490\tLoss: 0.016183\n",
      "\n",
      "Train time for epoch #3 (global step 3000): 98.045462\n",
      "Validation set: Average loss: 0.1348, Accuracy: 94.800000%\n",
      "\n",
      "Batch #0\tLoss: 0.128101\n",
      "Batch #10\tLoss: 0.019217\n",
      "Batch #20\tLoss: 0.094356\n",
      "Batch #30\tLoss: 0.167904\n",
      "Batch #40\tLoss: 0.181120\n",
      "Batch #50\tLoss: 0.015863\n",
      "Batch #60\tLoss: 0.022981\n",
      "Batch #70\tLoss: 0.115493\n",
      "Batch #80\tLoss: 0.016445\n",
      "Batch #90\tLoss: 0.031428\n",
      "Batch #100\tLoss: 0.037854\n",
      "Batch #110\tLoss: 0.009989\n",
      "Batch #120\tLoss: 0.171526\n",
      "Batch #130\tLoss: 0.062716\n",
      "Batch #140\tLoss: 0.025547\n",
      "Batch #150\tLoss: 0.074487\n",
      "Batch #160\tLoss: 0.133318\n",
      "Batch #170\tLoss: 0.036381\n",
      "Batch #180\tLoss: 0.161210\n",
      "Batch #190\tLoss: 0.049623\n",
      "Batch #200\tLoss: 0.122553\n",
      "Batch #210\tLoss: 0.035674\n",
      "Batch #220\tLoss: 0.086616\n",
      "Batch #230\tLoss: 0.044804\n",
      "Batch #240\tLoss: 0.053103\n",
      "Batch #250\tLoss: 0.060756\n",
      "Batch #260\tLoss: 0.091845\n",
      "Batch #270\tLoss: 0.040231\n",
      "Batch #280\tLoss: 0.113456\n",
      "Batch #290\tLoss: 0.203792\n",
      "Batch #300\tLoss: 0.062176\n",
      "Batch #310\tLoss: 0.022904\n",
      "Batch #320\tLoss: 0.062331\n",
      "Batch #330\tLoss: 0.065141\n",
      "Batch #340\tLoss: 0.080470\n",
      "Batch #350\tLoss: 0.044368\n",
      "Batch #360\tLoss: 0.025237\n",
      "Batch #370\tLoss: 0.111603\n",
      "Batch #380\tLoss: 0.004607\n",
      "Batch #390\tLoss: 0.038392\n",
      "Batch #400\tLoss: 0.109545\n",
      "Batch #410\tLoss: 0.103351\n",
      "Batch #420\tLoss: 0.076816\n",
      "Batch #430\tLoss: 0.027740\n",
      "Batch #440\tLoss: 0.138348\n",
      "Batch #450\tLoss: 0.017414\n",
      "Batch #460\tLoss: 0.123366\n",
      "Batch #470\tLoss: 0.047600\n",
      "Batch #480\tLoss: 0.218181\n",
      "Batch #490\tLoss: 0.063436\n",
      "\n",
      "Train time for epoch #4 (global step 3500): 94.173608\n",
      "Validation set: Average loss: 0.1139, Accuracy: 95.800000%\n",
      "\n",
      "Batch #0\tLoss: 0.005870\n",
      "Batch #10\tLoss: 0.010403\n",
      "Batch #20\tLoss: 0.019528\n",
      "Batch #30\tLoss: 0.160095\n",
      "Batch #40\tLoss: 0.041443\n",
      "Batch #50\tLoss: 0.208060\n",
      "Batch #60\tLoss: 0.020245\n",
      "Batch #70\tLoss: 0.086028\n",
      "Batch #80\tLoss: 0.004527\n",
      "Batch #90\tLoss: 0.108995\n",
      "Batch #100\tLoss: 0.159076\n",
      "Batch #110\tLoss: 0.027756\n",
      "Batch #120\tLoss: 0.104874\n",
      "Batch #130\tLoss: 0.103419\n",
      "Batch #140\tLoss: 0.010084\n",
      "Batch #150\tLoss: 0.118089\n",
      "Batch #160\tLoss: 0.098893\n",
      "Batch #170\tLoss: 0.063448\n",
      "Batch #180\tLoss: 0.293384\n",
      "Batch #190\tLoss: 0.167353\n",
      "Batch #200\tLoss: 0.098500\n",
      "Batch #210\tLoss: 0.020274\n",
      "Batch #220\tLoss: 0.172121\n",
      "Batch #230\tLoss: 0.038616\n",
      "Batch #240\tLoss: 0.062351\n",
      "Batch #250\tLoss: 0.030875\n",
      "Batch #260\tLoss: 0.164586\n",
      "Batch #270\tLoss: 0.102710\n",
      "Batch #280\tLoss: 0.116778\n",
      "Batch #290\tLoss: 0.044485\n",
      "Batch #300\tLoss: 0.033866\n",
      "Batch #310\tLoss: 0.026601\n",
      "Batch #320\tLoss: 0.010997\n",
      "Batch #330\tLoss: 0.074946\n",
      "Batch #340\tLoss: 0.140110\n",
      "Batch #350\tLoss: 0.120411\n",
      "Batch #360\tLoss: 0.295303\n",
      "Batch #370\tLoss: 0.093738\n",
      "Batch #380\tLoss: 0.099270\n",
      "Batch #390\tLoss: 0.082218\n",
      "Batch #400\tLoss: 0.045982\n",
      "Batch #410\tLoss: 0.238710\n",
      "Batch #420\tLoss: 0.069881\n",
      "Batch #430\tLoss: 0.123437\n",
      "Batch #440\tLoss: 0.108430\n",
      "Batch #450\tLoss: 0.104047\n",
      "Batch #460\tLoss: 0.227127\n",
      "Batch #470\tLoss: 0.047938\n",
      "Batch #480\tLoss: 0.154237\n",
      "Batch #490\tLoss: 0.175706\n",
      "\n",
      "Train time for epoch #5 (global step 4000): 97.560053\n",
      "Validation set: Average loss: 0.1145, Accuracy: 95.200000%\n",
      "\n",
      "Batch #0\tLoss: 0.059437\n",
      "Batch #10\tLoss: 0.031582\n",
      "Batch #20\tLoss: 0.064668\n",
      "Batch #30\tLoss: 0.143576\n",
      "Batch #40\tLoss: 0.059140\n",
      "Batch #50\tLoss: 0.056801\n",
      "Batch #60\tLoss: 0.009853\n",
      "Batch #70\tLoss: 0.006888\n",
      "Batch #80\tLoss: 0.130857\n",
      "Batch #90\tLoss: 0.025234\n",
      "Batch #100\tLoss: 0.054970\n",
      "Batch #110\tLoss: 0.020822\n",
      "Batch #120\tLoss: 0.055169\n",
      "Batch #130\tLoss: 0.046373\n",
      "Batch #140\tLoss: 0.079403\n",
      "Batch #150\tLoss: 0.006324\n",
      "Batch #160\tLoss: 0.035471\n",
      "Batch #170\tLoss: 0.075645\n",
      "Batch #180\tLoss: 0.061321\n",
      "Batch #190\tLoss: 0.081382\n",
      "Batch #200\tLoss: 0.154414\n",
      "Batch #210\tLoss: 0.153648\n",
      "Batch #220\tLoss: 0.152347\n",
      "Batch #230\tLoss: 0.007192\n",
      "Batch #240\tLoss: 0.093146\n",
      "Batch #250\tLoss: 0.146242\n",
      "Batch #260\tLoss: 0.016019\n",
      "Batch #270\tLoss: 0.023047\n",
      "Batch #280\tLoss: 0.156532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch #290\tLoss: 0.082214\n",
      "Batch #300\tLoss: 0.011540\n",
      "Batch #310\tLoss: 0.186713\n",
      "Batch #320\tLoss: 0.089462\n",
      "Batch #330\tLoss: 0.142778\n",
      "Batch #340\tLoss: 0.042163\n",
      "Batch #350\tLoss: 0.116340\n",
      "Batch #360\tLoss: 0.047809\n",
      "Batch #370\tLoss: 0.035371\n",
      "Batch #380\tLoss: 0.195572\n",
      "Batch #390\tLoss: 0.131415\n",
      "Batch #400\tLoss: 0.021083\n",
      "Batch #410\tLoss: 0.009651\n",
      "Batch #420\tLoss: 0.065956\n",
      "Batch #430\tLoss: 0.080049\n",
      "Batch #440\tLoss: 0.014586\n",
      "Batch #450\tLoss: 0.125319\n",
      "Batch #460\tLoss: 0.117395\n",
      "Batch #470\tLoss: 0.207680\n",
      "Batch #480\tLoss: 0.137758\n",
      "Batch #490\tLoss: 0.036870\n",
      "\n",
      "Train time for epoch #6 (global step 4500): 94.620126\n",
      "Validation set: Average loss: 0.1345, Accuracy: 94.600000%\n",
      "\n",
      "Batch #0\tLoss: 0.035587\n",
      "Batch #10\tLoss: 0.129208\n",
      "Batch #20\tLoss: 0.019319\n",
      "Batch #30\tLoss: 0.108306\n",
      "Batch #40\tLoss: 0.030628\n",
      "Batch #50\tLoss: 0.018305\n",
      "Batch #60\tLoss: 0.031809\n",
      "Batch #70\tLoss: 0.030856\n",
      "Batch #80\tLoss: 0.118930\n",
      "Batch #90\tLoss: 0.010142\n",
      "Batch #100\tLoss: 0.023664\n",
      "Batch #110\tLoss: 0.055607\n",
      "Batch #120\tLoss: 0.019184\n",
      "Batch #130\tLoss: 0.063982\n",
      "Batch #140\tLoss: 0.016987\n",
      "Batch #150\tLoss: 0.187068\n",
      "Batch #160\tLoss: 0.058712\n",
      "Batch #170\tLoss: 0.073939\n",
      "Batch #180\tLoss: 0.062766\n",
      "Batch #190\tLoss: 0.042537\n",
      "Batch #200\tLoss: 0.051048\n",
      "Batch #210\tLoss: 0.004484\n",
      "Batch #220\tLoss: 0.012202\n",
      "Batch #230\tLoss: 0.011908\n",
      "Batch #240\tLoss: 0.081249\n",
      "Batch #250\tLoss: 0.022512\n",
      "Batch #260\tLoss: 0.202008\n",
      "Batch #270\tLoss: 0.077773\n",
      "Batch #280\tLoss: 0.035895\n",
      "Batch #290\tLoss: 0.203098\n",
      "Batch #300\tLoss: 0.062140\n",
      "Batch #310\tLoss: 0.019770\n",
      "Batch #320\tLoss: 0.031555\n",
      "Batch #330\tLoss: 0.038971\n",
      "Batch #340\tLoss: 0.027229\n",
      "Batch #350\tLoss: 0.085488\n",
      "Batch #360\tLoss: 0.078515\n",
      "Batch #370\tLoss: 0.066502\n",
      "Batch #380\tLoss: 0.041610\n",
      "Batch #390\tLoss: 0.087074\n",
      "Batch #400\tLoss: 0.117079\n",
      "Batch #410\tLoss: 0.110885\n",
      "Batch #420\tLoss: 0.110282\n",
      "Batch #430\tLoss: 0.124123\n",
      "Batch #440\tLoss: 0.077111\n",
      "Batch #450\tLoss: 0.025085\n",
      "Batch #460\tLoss: 0.019961\n",
      "Batch #470\tLoss: 0.010968\n",
      "Batch #480\tLoss: 0.034931\n",
      "Batch #490\tLoss: 0.060048\n",
      "\n",
      "Train time for epoch #7 (global step 5000): 90.308219\n",
      "Validation set: Average loss: 0.1295, Accuracy: 95.400000%\n",
      "\n",
      "Batch #0\tLoss: 0.068917\n",
      "Batch #10\tLoss: 0.070027\n",
      "Batch #20\tLoss: 0.005392\n",
      "Batch #30\tLoss: 0.018308\n",
      "Batch #40\tLoss: 0.073455\n",
      "Batch #50\tLoss: 0.350896\n",
      "Batch #60\tLoss: 0.034240\n",
      "Batch #70\tLoss: 0.041250\n",
      "Batch #80\tLoss: 0.194807\n",
      "Batch #90\tLoss: 0.186180\n",
      "Batch #100\tLoss: 0.059084\n",
      "Batch #110\tLoss: 0.065126\n",
      "Batch #120\tLoss: 0.128834\n",
      "Batch #130\tLoss: 0.089205\n",
      "Batch #140\tLoss: 0.052563\n",
      "Batch #150\tLoss: 0.038012\n",
      "Batch #160\tLoss: 0.155524\n",
      "Batch #170\tLoss: 0.020714\n",
      "Batch #180\tLoss: 0.007914\n",
      "Batch #190\tLoss: 0.137818\n",
      "Batch #200\tLoss: 0.070967\n",
      "Batch #210\tLoss: 0.079657\n",
      "Batch #220\tLoss: 0.023394\n",
      "Batch #230\tLoss: 0.140109\n",
      "Batch #240\tLoss: 0.013085\n",
      "Batch #250\tLoss: 0.267137\n",
      "Batch #260\tLoss: 0.127880\n",
      "Batch #270\tLoss: 0.032661\n",
      "Batch #280\tLoss: 0.139623\n",
      "Batch #290\tLoss: 0.054405\n",
      "Batch #300\tLoss: 0.101408\n",
      "Batch #310\tLoss: 0.065887\n",
      "Batch #320\tLoss: 0.077343\n",
      "Batch #330\tLoss: 0.024727\n",
      "Batch #340\tLoss: 0.074577\n",
      "Batch #350\tLoss: 0.026064\n",
      "Batch #360\tLoss: 0.118194\n",
      "Batch #370\tLoss: 0.059932\n",
      "Batch #380\tLoss: 0.027052\n",
      "Batch #390\tLoss: 0.024190\n",
      "Batch #400\tLoss: 0.033353\n",
      "Batch #410\tLoss: 0.053094\n",
      "Batch #420\tLoss: 0.028250\n",
      "Batch #430\tLoss: 0.141061\n",
      "Batch #440\tLoss: 0.048363\n",
      "Batch #450\tLoss: 0.201465\n",
      "Batch #460\tLoss: 0.042198\n",
      "Batch #470\tLoss: 0.068162\n",
      "Batch #480\tLoss: 0.252129\n",
      "Batch #490\tLoss: 0.049711\n",
      "\n",
      "Train time for epoch #8 (global step 5500): 91.812561\n",
      "Validation set: Average loss: 0.0850, Accuracy: 96.800000%\n",
      "\n",
      "Batch #0\tLoss: 0.067185\n",
      "Batch #10\tLoss: 0.007954\n",
      "Batch #20\tLoss: 0.045736\n",
      "Batch #30\tLoss: 0.020864\n",
      "Batch #40\tLoss: 0.049186\n",
      "Batch #50\tLoss: 0.106578\n",
      "Batch #60\tLoss: 0.095101\n",
      "Batch #70\tLoss: 0.034868\n",
      "Batch #80\tLoss: 0.334494\n",
      "Batch #90\tLoss: 0.055756\n",
      "Batch #100\tLoss: 0.097451\n",
      "Batch #110\tLoss: 0.088074\n",
      "Batch #120\tLoss: 0.113724\n",
      "Batch #130\tLoss: 0.109219\n",
      "Batch #140\tLoss: 0.175167\n",
      "Batch #150\tLoss: 0.138801\n",
      "Batch #160\tLoss: 0.051168\n",
      "Batch #170\tLoss: 0.005301\n",
      "Batch #180\tLoss: 0.010081\n",
      "Batch #190\tLoss: 0.015868\n",
      "Batch #200\tLoss: 0.172833\n",
      "Batch #210\tLoss: 0.042459\n",
      "Batch #220\tLoss: 0.057315\n",
      "Batch #230\tLoss: 0.100241\n",
      "Batch #240\tLoss: 0.030860\n",
      "Batch #250\tLoss: 0.047047\n",
      "Batch #260\tLoss: 0.006709\n",
      "Batch #270\tLoss: 0.030798\n",
      "Batch #280\tLoss: 0.091084\n",
      "Batch #290\tLoss: 0.060224\n",
      "Batch #300\tLoss: 0.024889\n",
      "Batch #310\tLoss: 0.038046\n",
      "Batch #320\tLoss: 0.107851\n",
      "Batch #330\tLoss: 0.007153\n",
      "Batch #340\tLoss: 0.018166\n",
      "Batch #350\tLoss: 0.048606\n",
      "Batch #360\tLoss: 0.146453\n",
      "Batch #370\tLoss: 0.074962\n",
      "Batch #380\tLoss: 0.029152\n",
      "Batch #390\tLoss: 0.030228\n",
      "Batch #400\tLoss: 0.066513\n",
      "Batch #410\tLoss: 0.026964\n",
      "Batch #420\tLoss: 0.032964\n",
      "Batch #430\tLoss: 0.372690\n",
      "Batch #440\tLoss: 0.126469\n",
      "Batch #450\tLoss: 0.011725\n",
      "Batch #460\tLoss: 0.011276\n",
      "Batch #470\tLoss: 0.102064\n",
      "Batch #480\tLoss: 0.049193\n",
      "Batch #490\tLoss: 0.061330\n",
      "\n",
      "Train time for epoch #9 (global step 6000): 92.475626\n",
      "Validation set: Average loss: 0.1195, Accuracy: 95.600000%\n",
      "\n",
      "Batch #0\tLoss: 0.053542\n",
      "Batch #10\tLoss: 0.065034\n",
      "Batch #20\tLoss: 0.022986\n",
      "Batch #30\tLoss: 0.094957\n",
      "Batch #40\tLoss: 0.117798\n",
      "Batch #50\tLoss: 0.051192\n",
      "Batch #60\tLoss: 0.007728\n",
      "Batch #70\tLoss: 0.102291\n",
      "Batch #80\tLoss: 0.070248\n",
      "Batch #90\tLoss: 0.041513\n",
      "Batch #100\tLoss: 0.001998\n",
      "Batch #110\tLoss: 0.133282\n",
      "Batch #120\tLoss: 0.002870\n",
      "Batch #130\tLoss: 0.110039\n",
      "Batch #140\tLoss: 0.052714\n",
      "Batch #150\tLoss: 0.194445\n",
      "Batch #160\tLoss: 0.020569\n",
      "Batch #170\tLoss: 0.052748\n",
      "Batch #180\tLoss: 0.308852\n",
      "Batch #190\tLoss: 0.281845\n",
      "Batch #200\tLoss: 0.063496\n",
      "Batch #210\tLoss: 0.150581\n",
      "Batch #220\tLoss: 0.079984\n",
      "Batch #230\tLoss: 0.045020\n",
      "Batch #240\tLoss: 0.086357\n",
      "Batch #250\tLoss: 0.116620\n",
      "Batch #260\tLoss: 0.116348\n",
      "Batch #270\tLoss: 0.079332\n",
      "Batch #280\tLoss: 0.188059\n",
      "Batch #290\tLoss: 0.033977\n",
      "Batch #300\tLoss: 0.075733\n",
      "Batch #310\tLoss: 0.028093\n",
      "Batch #320\tLoss: 0.142050\n",
      "Batch #330\tLoss: 0.012455\n",
      "Batch #340\tLoss: 0.079717\n",
      "Batch #350\tLoss: 0.033259\n",
      "Batch #360\tLoss: 0.013449\n",
      "Batch #370\tLoss: 0.119143\n",
      "Batch #380\tLoss: 0.022265\n",
      "Batch #390\tLoss: 0.060394\n",
      "Batch #400\tLoss: 0.055551\n",
      "Batch #410\tLoss: 0.143016\n",
      "Batch #420\tLoss: 0.079012\n",
      "Batch #430\tLoss: 0.028572\n",
      "Batch #440\tLoss: 0.035963\n",
      "Batch #450\tLoss: 0.035026\n",
      "Batch #460\tLoss: 0.097812\n",
      "Batch #470\tLoss: 0.017907\n",
      "Batch #480\tLoss: 0.029136\n",
      "Batch #490\tLoss: 0.077742\n",
      "\n",
      "Train time for epoch #10 (global step 6500): 93.833773\n",
      "Validation set: Average loss: 0.1027, Accuracy: 97.000000%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. train and save\n",
    "training_accuracy,training_loss,valid_accuracy,valid_loss = NeuralPlanner.train(X_train,y_train,X_valid,y_valid,batch_size=opt.minibatch_size,n_minibatches=opt.n_minibatches,\n",
    "                    num_epochs=10,learning_rate=1e-3,dataset_size=None,log_interval=10,no_gpu=False,checkpoint_dir=opt.checkpoint_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
